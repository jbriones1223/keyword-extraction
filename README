Jack Briones
SURF 2018
Alvarez


FILES:
grabber.py - 
    Basic script to pull text from a file which is specified inside the code.
    This script implements two basic keyword extraction methods: POS tagging and
    selectivity. The POS tagging approach selects the most frequent nouns, while
    the selectivity approach chooses words based on adjacency within a sentence.
    In both cases, stopwords are removed, but this really only effects the
    selectivity approach, because the POS tagging only selects nouns anyway.

grabber2.py -
    Similar to grabber.py, with a different input file. The file should be a csv
    containing the full content of tweets, minus any non-ascii characters. Note
    that this is currently very slow for large datasets. It took a few minutes
    to run on 10,000 tweets. This correlates to the size of the .csv file, not
    just how many tweets within the file we eliminate. For this reason, the file
    was generated with:

        > dat <- readRDS(rds_file)
        > write.csv(lapply(dat$full_content[1:10000], fixUTF), csv_file)

    I'm going to see what I can do to speed it up, but for now just be aware of
    the amount of data.
    
    fixUTF has the following definition:

        fixUTF <- function(tweet){
            tweetInt <- utf8ToInt(tweet)
            probInt <- which(tweetInt > 127)
            if(length(probInt) > 0){
                tweetInt <- tweetInt[-probInt]
            }
            return(intToUtf8(tweetInt))
        }
        
    Note that grabber2_j5.py is the same file, with the main difference being
    that the initial keywords from the Election Day data set are included.

grabber3.py -
    the most updated version of the previous 2 files

structure.py -
    this script first scans through the tweets to collect a list of 'contexts'
    which appear around the keywords used for the search. These contexts are
    lists of words, with '<|>' representing the keyword. Next, the script scans
    through the tweets again, finding which other phrases appear in those
    contexts. Each of these phrases is scored, and the top-scoring phrases are
    returned.

context.py -
    Work in progress. The basic idea is a complementary n-gram frequency
    metric. Given a set of phrases, scan through the tweets, finding which
    `contexts' occur around each phrase. The top n contexts are printed as a
    result, but it shouldn't be too hard to use this as input for another
    program.

topic_model.py -
    This runs an LDA (Latent Dirichlet Allocation) on the set of tweets, and
    it returns a topic model.

track_changes.py -
    This analyzes several algorithm outputs to determine how the keywords
    within the results change over time. In test runs, I ran this with the
    election day data split chronologically into 5 chunks.

twitter_as_data.txt -
    Section 1 of the Twitter as Data article by Professor Alvarez. Medium-sized
    text file for testing. Note that at this point, this is mainly for error
    checking, as only including the first section will eliminate some potential
    keywords occurring in other sections.




MOVING FORWARD:
    - consider adding other word types to the POS approach
    - deal with parentheticals in a sentence (possibly split it into two
        different sentences?
    - implement other approaches, now that the basics are implemented


R expressions:
dfraud <- dat$full_content[which(dat$fraud)]
